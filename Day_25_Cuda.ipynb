{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiLazn3Wc8rL"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y nvidia-cuda-toolkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vector_add.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <curand_kernel.h>\n",
        "\n",
        "#define NUM_SIMULATIONS 1024  // Number of rollouts\n",
        "#define MAX_DEPTH 100         // Maximum rollout depth\n",
        "\n",
        "// Define the game state\n",
        "struct GameState {\n",
        "    int moves[10];\n",
        "    int num_moves;\n",
        "    bool is_terminal;\n",
        "    float reward;  // Reward if terminal\n",
        "\n",
        "    __device__ GameState next_state(int action) {\n",
        "        GameState new_state = *this;\n",
        "        // Apply the action\n",
        "        new_state.reward += (action % 2 == 0) ? 1.0f : -1.0f;\n",
        "        new_state.is_terminal = (new_state.reward > 10 || new_state.reward < -10);\n",
        "        return new_state;\n",
        "    }\n",
        "\n",
        "    __device__ int get_random_action(curandState* state) {\n",
        "        if (num_moves == 0) return -1;\n",
        "        return moves[curand(state) % num_moves];\n",
        "    }\n",
        "};\n",
        "\n",
        "// Node structure for MCTS\n",
        "struct Node {\n",
        "    GameState state;\n",
        "    int visits;\n",
        "    float value;\n",
        "};\n",
        "\n",
        "// Device function for rollout (Simulation phase)\n",
        "__device__ float rollout(GameState state, curandState* rand_state) {\n",
        "    int depth = 0;\n",
        "    while (!state.is_terminal && depth < MAX_DEPTH) {\n",
        "        int action = state.get_random_action(rand_state);\n",
        "        if (action == -1) break;  // No moves available\n",
        "        state = state.next_state(action);\n",
        "        depth++;\n",
        "    }\n",
        "    return state.reward;\n",
        "}\n",
        "\n",
        "// Kernel to run parallel rollouts\n",
        "__global__ void mcts_kernel(Node* nodes, int num_nodes, float* results) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (idx >= num_nodes) return;\n",
        "\n",
        "    curandState rand_state;\n",
        "    curand_init(idx, 0, 0, &rand_state);\n",
        "\n",
        "    float total_reward = 0;\n",
        "    for (int i = 0; i < NUM_SIMULATIONS; i++) {\n",
        "        total_reward += rollout(nodes[idx].state, &rand_state);\n",
        "    }\n",
        "\n",
        "    results[idx] = total_reward / NUM_SIMULATIONS;\n",
        "}\n",
        "\n",
        "// Host function to execute MCTS\n",
        "void run_mcts(Node* host_nodes, int num_nodes) {\n",
        "    Node* device_nodes;\n",
        "    float* device_results;\n",
        "    float* host_results = (float*)malloc(num_nodes * sizeof(float));\n",
        "\n",
        "    cudaMalloc(&device_nodes, num_nodes * sizeof(Node));\n",
        "    cudaMalloc(&device_results, num_nodes * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(device_nodes, host_nodes, num_nodes * sizeof(Node), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (num_nodes + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    mcts_kernel<<<blocksPerGrid, threadsPerBlock>>>(device_nodes, num_nodes, device_results);\n",
        "\n",
        "    cudaMemcpy(host_results, device_results, num_nodes * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Update values in host nodes\n",
        "    for (int i = 0; i < num_nodes; i++) {\n",
        "        host_nodes[i].value = host_results[i];\n",
        "    }\n",
        "\n",
        "    free(host_results);\n",
        "    cudaFree(device_nodes);\n",
        "    cudaFree(device_results);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Create root node with example state\n",
        "    Node root;\n",
        "    root.state.num_moves = 10;\n",
        "    root.state.is_terminal = false;\n",
        "    root.visits = 0;\n",
        "    root.value = 0;\n",
        "\n",
        "    run_mcts(&root, 1);\n",
        "\n",
        "    printf(\"MCTS result: %f\\n\", root.value);\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "QzFC1iDadU_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./vector_add"
      ],
      "metadata": {
        "id": "J2IViKhPdWKF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}