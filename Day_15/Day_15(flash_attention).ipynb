{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms5M9PTD6XYZ"
      },
      "outputs": [],
      "source": [
        "%%writefile vector_add.cu\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "#include <cstdlib>\n",
        "#define CUDA_MAX_NUM_THREADS 1024\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// CUDA Kernel for computing dL/dW\n",
        "template <typename T>\n",
        "__global__ void compute_dLdW(T* dLdY, T* input_unrolled, T* dLdW, int output_height, int output_width, int num_filters, int filter_size) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < filter_size && col < num_filters) {\n",
        "        T sum = 0;\n",
        "        for (int i = 0; i < output_height * output_width; i++) {\n",
        "            sum += input_unrolled[i * filter_size + row] * dLdY[i * num_filters + col];\n",
        "        }\n",
        "        dLdW[row * num_filters + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA Kernel for computing dL/dX\n",
        "template <typename T>\n",
        "__global__ void compute_dLdX(T* dLdY, T* weights, T* dLdX_unrolled, int output_height, int output_width, int num_filters, int filter_size) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < output_height * output_width && col < filter_size) {\n",
        "        T sum = 0;\n",
        "        for (int i = 0; i < num_filters; i++) {\n",
        "            sum += dLdY[row * num_filters + i] * weights[col * num_filters + i];\n",
        "        }\n",
        "        dLdX_unrolled[row * filter_size + col] = sum;\n",
        "    }\n",
        "}\n",
        "template <typename T>\n",
        "__global__ void maxPoolingBackwardKernel(T* dLdY, T* input, T* dLdX, int input_height, int input_width, int pool_size, int stride) {\n",
        "    int output_height = (input_height - pool_size) / stride + 1;\n",
        "    int output_width = (input_width - pool_size) / stride + 1;\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < output_height && col < output_width) {\n",
        "        T max_value = -INFINITY;\n",
        "        int max_i = -1, max_j = -1;\n",
        "        for (int i = 0; i < pool_size; i++) {\n",
        "            for (int j = 0; j < pool_size; j++) {\n",
        "                int input_row = row * stride + i;\n",
        "                int input_col = col * stride + j;\n",
        "\n",
        "                // Access input correctly, avoid out-of-bounds access\n",
        "                if (input_row < input_height && input_col < input_width) {\n",
        "                    if (input[input_row * input_width + input_col] > max_value) {\n",
        "                        max_value = input[input_row * input_width + input_col];\n",
        "                        max_i = input_row;\n",
        "                        max_j = input_col;\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Ensure max_i and max_j are valid before accessing dLdX\n",
        "        if (max_i != -1 && max_j != -1) {\n",
        "            atomicAdd(&dLdX[max_i * input_width + max_j], dLdY[row * output_width + col]);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Updated kernel signatures to match the calling convention\n",
        "__global__ void unrollKernel(const float* input, float* input_unrolled,\n",
        "                            const int input_channels, const int input_height, const int input_width,\n",
        "                            const int kernel_size, const int output_height, const int output_width) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total_elements = output_height * output_width;\n",
        "\n",
        "    if (idx < total_elements) {\n",
        "        int out_y = idx / output_width;\n",
        "        int out_x = idx % output_width;\n",
        "\n",
        "        for (int c = 0; c < input_channels; c++) {\n",
        "            for (int ky = 0; ky < kernel_size; ky++) {\n",
        "                for (int kx = 0; kx < kernel_size; kx++) {\n",
        "                    int in_y = out_y + ky;\n",
        "                    int in_x = out_x + kx;\n",
        "\n",
        "                    int unroll_idx = idx * (input_channels * kernel_size * kernel_size) +\n",
        "                                   (c * kernel_size * kernel_size + ky * kernel_size + kx);\n",
        "\n",
        "                    int input_idx = c * (input_height * input_width) +\n",
        "                                  in_y * input_width + in_x;\n",
        "\n",
        "                    input_unrolled[unroll_idx] = input[input_idx];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// Host function to launch Unrolling Kernel\n",
        "void unrollInput(int input_channels, int input_height, int input_width,\n",
        "                int kernel_size, float* input, float* input_unrolled) {\n",
        "    int output_height = input_height - kernel_size + 1;\n",
        "    int output_width = input_width - kernel_size + 1;\n",
        "    int total_output_elements = output_height * output_width;\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int numBlocks = (total_output_elements + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    unrollKernel<<<numBlocks, threadsPerBlock>>>(\n",
        "        input,                  // const float* input\n",
        "        input_unrolled,        // float* input_unrolled\n",
        "        input_channels,        // const int input_channels\n",
        "        input_height,          // const int input_height\n",
        "        input_width,           // const int input_width\n",
        "        kernel_size,           // const int kernel_size\n",
        "        output_height,         // const int output_height\n",
        "        output_width          // const int output_width\n",
        "    );\n",
        "\n",
        "    cudaError_t error = cudaGetLastError();\n",
        "    if (error != cudaSuccess) {\n",
        "        printf(\"CUDA error in unroll: %s\\n\", cudaGetErrorString(error));\n",
        "    }\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "}\n",
        "\n",
        "void convolutionBackward(int batch_size, int num_filters, int input_channels, int input_height, int input_width, int kernel_size, float* dLdY, float* input, float* weights, float* dLdX, float* dLdW) {\n",
        "    int output_height = input_height - kernel_size + 1;\n",
        "    int output_width = input_width - kernel_size + 1;\n",
        "    int filter_size = input_channels * kernel_size * kernel_size;\n",
        "\n",
        "    float* input_unrolled;\n",
        "    float* dLdX_unrolled;\n",
        "    cudaMalloc(&input_unrolled, output_height * output_width * filter_size * sizeof(float));\n",
        "    cudaMalloc(&dLdX_unrolled, output_height * output_width * filter_size * sizeof(float));\n",
        "\n",
        "    for (int n = 0; n < batch_size; n++) {\n",
        "        unrollInput(input_channels, input_height, input_width, kernel_size, input + n * input_channels * input_height * input_width, input_unrolled);\n",
        "\n",
        "        dim3 blockSize(16, 16);\n",
        "        dim3 gridSize((output_width + blockSize.x - 1) / blockSize.x, (output_height + blockSize.y - 1) / blockSize.y);\n",
        "\n",
        "        compute_dLdW<<<gridSize, blockSize>>>(dLdY, input_unrolled, dLdW, output_height, output_width, num_filters, filter_size);\n",
        "        compute_dLdX<<<gridSize, blockSize>>>(dLdY, weights, dLdX_unrolled, output_height, output_width, num_filters, filter_size);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    cudaFree(input_unrolled);\n",
        "    cudaFree(dLdX_unrolled);\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "// CUDA Kernel for Max Pooling\n",
        "__global__ void maxPoolingKernel(float* input, float* output, int input_height, int input_width, int pool_size, int stride) {\n",
        "    int output_height = (input_height - pool_size) / stride + 1;\n",
        "    int output_width = (input_width - pool_size) / stride + 1;\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < output_height && col < output_width) {\n",
        "        float max_value = -INFINITY;\n",
        "        for (int i = 0; i < pool_size; i++) {\n",
        "            for (int j = 0; j < pool_size; j++) {\n",
        "                int input_row = row * stride + i;\n",
        "                int input_col = col * stride + j;\n",
        "                max_value = fmaxf(max_value, input[input_row * input_width + input_col]);\n",
        "            }\n",
        "        }\n",
        "        output[row * output_width + col] = max_value;\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA Kernel for Matrix Multiplication (GEMM for Convolution)\n",
        "__global__ void matrixMultiplicationKernel(float* input_unrolled, float* weights, float* output,\n",
        "                                         int output_height, int output_width, int num_filters, int filter_size) {\n",
        "    // Calculate actual position\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total_output_elements = output_height * output_width;\n",
        "\n",
        "    if (idx < total_output_elements * num_filters) {\n",
        "        int output_idx = idx / num_filters;  // Position in output feature map\n",
        "        int filter_idx = idx % num_filters;  // Which filter we're using\n",
        "\n",
        "        float sum = 0.0f;\n",
        "        // Multiply unrolled input with the corresponding filter\n",
        "        for (int i = 0; i < filter_size; i++) {\n",
        "            sum += input_unrolled[output_idx * filter_size + i] * weights[i * num_filters + filter_idx];\n",
        "        }\n",
        "        output[idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "__global__ void convolutionKernel(const float* input_unrolled, const float* weights, float* output,\n",
        "                                 const int output_size, const int num_filters, const int filter_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < output_size * num_filters) {\n",
        "        int output_idx = idx / num_filters;\n",
        "        int filter_idx = idx % num_filters;\n",
        "\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < filter_size; i++) {\n",
        "            sum += input_unrolled[output_idx * filter_size + i] *\n",
        "                   weights[i * num_filters + filter_idx];\n",
        "        }\n",
        "        output[idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "void convolutionForward(float* input, float* weights, float* output,\n",
        "                       int batch_size, int num_filters, int input_channels,\n",
        "                       int input_height, int input_width, int kernel_size) {\n",
        "    int output_height = input_height - kernel_size + 1;\n",
        "    int output_width = input_width - kernel_size + 1;\n",
        "    int output_size = output_height * output_width;\n",
        "    int filter_size = input_channels * kernel_size * kernel_size;\n",
        "\n",
        "    // Allocate unrolled input matrix\n",
        "    float* input_unrolled;\n",
        "    size_t unrolled_size = output_size * filter_size * sizeof(float);\n",
        "    cudaMalloc(&input_unrolled, unrolled_size);\n",
        "\n",
        "    // Calculate grid and block dimensions\n",
        "    int unroll_blocks = (output_size + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "    int conv_blocks = (output_size * num_filters + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    for (int n = 0; n < batch_size; n++) {\n",
        "        float* input_n = input + n * input_channels * input_height * input_width;\n",
        "        float* output_n = output + n * num_filters * output_height * output_width;\n",
        "\n",
        "        // Launch unroll kernel with correct parameters\n",
        "        unrollKernel<<<unroll_blocks, BLOCK_SIZE>>>(\n",
        "            input_n,\n",
        "            input_unrolled,\n",
        "            input_channels,\n",
        "            input_height,\n",
        "            input_width,\n",
        "            kernel_size,\n",
        "            output_height,\n",
        "            output_width\n",
        "        );\n",
        "\n",
        "        // Check for kernel launch errors\n",
        "        cudaError_t error = cudaGetLastError();\n",
        "        if (error != cudaSuccess) {\n",
        "            printf(\"Unroll kernel error: %s\\n\", cudaGetErrorString(error));\n",
        "        }\n",
        "\n",
        "        // Launch convolution kernel\n",
        "        convolutionKernel<<<conv_blocks, BLOCK_SIZE>>>(\n",
        "            input_unrolled,\n",
        "            weights,\n",
        "            output_n,\n",
        "            output_size,\n",
        "            num_filters,\n",
        "            filter_size\n",
        "        );\n",
        "\n",
        "        // Check for kernel launch errors\n",
        "        error = cudaGetLastError();\n",
        "        if (error != cudaSuccess) {\n",
        "            printf(\"Convolution kernel error: %s\\n\", cudaGetErrorString(error));\n",
        "        }\n",
        "\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    cudaFree(input_unrolled);\n",
        "}\n",
        "\n",
        "void testConvNet() {\n",
        "    // Test dimensions\n",
        "    const int batch_size = 1;\n",
        "    const int input_channels = 1;\n",
        "    const int input_height = 4;\n",
        "    const int input_width = 4;\n",
        "    const int kernel_size = 3;\n",
        "    const int num_filters = 2;\n",
        "\n",
        "    // Calculate output dimensions\n",
        "    const int output_height = input_height - kernel_size + 1;\n",
        "    const int output_width = input_width - kernel_size + 1;\n",
        "\n",
        "    // Allocate and initialize host memory\n",
        "    float input[] = {\n",
        "        1, 2, 3, 4,\n",
        "        5, 6, 7, 8,\n",
        "        9, 10, 11, 12,\n",
        "        13, 14, 15, 16\n",
        "    };\n",
        "\n",
        "    float weights[] = {\n",
        "        1, 0, -1,\n",
        "        1, 0, -1,\n",
        "        1, 0, -1,\n",
        "        // Second filter\n",
        "        0, 1, -1,\n",
        "        0, 1, -1,\n",
        "        0, 1, -1\n",
        "    };\n",
        "\n",
        "    // Allocate device memory\n",
        "    float *d_input, *d_weights, *d_output;\n",
        "\n",
        "    size_t input_size = batch_size * input_channels * input_height * input_width * sizeof(float);\n",
        "    size_t weights_size = num_filters * input_channels * kernel_size * kernel_size * sizeof(float);\n",
        "    size_t output_size = batch_size * num_filters * output_height * output_width * sizeof(float);\n",
        "\n",
        "    cudaMalloc(&d_input, input_size);\n",
        "    cudaMalloc(&d_weights, weights_size);\n",
        "    cudaMalloc(&d_output, output_size);\n",
        "    cudaMemset(d_output, 0, output_size);  // Initialize output to zero\n",
        "\n",
        "    // Copy data to device\n",
        "    cudaMemcpy(d_input, input, input_size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_weights, weights, weights_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Forward pass\n",
        "    convolutionForward(d_input, d_weights, d_output,\n",
        "                      batch_size, num_filters, input_channels,\n",
        "                      input_height, input_width, kernel_size);\n",
        "\n",
        "    // Copy results back to host\n",
        "    float* output = new float[output_size/sizeof(float)];\n",
        "    cudaMemcpy(output, d_output, output_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print results\n",
        "    std::cout << \"Forward Output:\\n\";\n",
        "    for (int f = 0; f < num_filters; f++) {\n",
        "        std::cout << \"Filter \" << f << \":\\n\";\n",
        "        for (int i = 0; i < output_height; i++) {\n",
        "            for (int j = 0; j < output_width; j++) {\n",
        "                std::cout << output[f * output_height * output_width + i * output_width + j] << \" \";\n",
        "            }\n",
        "            std::cout << \"\\n\";\n",
        "        }\n",
        "        std::cout << \"\\n\";\n",
        "    }\n",
        "\n",
        "    // Cleanup\n",
        "    delete[] output;\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_weights);\n",
        "    cudaFree(d_output);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    testConvNet();\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile with the specified architecture\n",
        "!nvcc vector_add.cu -o vector_add -gencode arch=compute_75,code=sm_75\n",
        "\n",
        "# Run the executable\n",
        "!./vector_add"
      ],
      "metadata": {
        "id": "TGzHCTnu6kKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ZJKKYCq6uq2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}