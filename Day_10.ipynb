{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D1upXhxmlvM"
      },
      "outputs": [],
      "source": [
        "%%writefile vector_add.cu\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "#include <random>\n",
        "#include <cmath>\n",
        "#include <curand.h>\n",
        "#include <fstream>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "__global__\n",
        "void forward_kernel(const float* query_matrix_device_pointer, const float* key_matrix_device_pointer, const float* value_matrix_device_pointer, const int sequence_length, const int embedding_dimension,\n",
        "                    const int total_columns_in_blocks, const int total_rows_in_blocks, const int block_size_columns, const int block_size_rows, const float softmax_scale,\n",
        "                    float* sum_matrix_device_pointer, float *max_matrix_device_pointer, float* output_matrix_device_pointer) {\n",
        "    int thread_index_x = threadIdx.x;\n",
        "    int block_index_x = blockIdx.x;\n",
        "    int block_index_y = blockIdx.y;  // batch and head index\n",
        "\n",
        "    // Offset into query_matrix_device_pointer,key_matrix_device_pointer,value_matrix_device_pointer,output_matrix_device_pointer,sum_matrix_device_pointer,max_matrix_device_pointer - different for each batch and head\n",
        "    int qkv_offset = (block_index_x * gridDim.y * sequence_length * embedding_dimension) + (block_index_y * sequence_length * embedding_dimension);  // gridDim.y = num_heads\n",
        "    int lm_offset = (block_index_x * gridDim.y * sequence_length) + (block_index_y * sequence_length);  // offset for sum_matrix_device_pointer and max_matrix_device_pointer\n",
        "\n",
        "    // Define SRAM for Q,K,V,S\n",
        "    extern __shared__ float shared_memory[];\n",
        "    int tile_size = block_size_columns * embedding_dimension;  // size of query_matrix_tile, key_matrix_tile, value_matrix_tile\n",
        "    float* query_matrix_tile = shared_memory;\n",
        "    float* key_matrix_tile = &shared_memory[tile_size];\n",
        "    float* value_matrix_tile = &shared_memory[tile_size * 2];\n",
        "    float* score_matrix_tile = &shared_memory[tile_size * 3];\n",
        "    float eps=1e-10;\n",
        "    for (int column_block_index = 0; column_block_index < total_columns_in_blocks; column_block_index++) {\n",
        "\n",
        "        // Load key_matrix_tile, value_matrix_tile to SRAM\n",
        "        for (int embedding_index = 0; embedding_index < embedding_dimension; embedding_index++) {\n",
        "            key_matrix_tile[(thread_index_x * embedding_dimension) + embedding_index] = key_matrix_device_pointer[qkv_offset + (tile_size * column_block_index) + (thread_index_x * embedding_dimension) + embedding_index];\n",
        "            value_matrix_tile[(thread_index_x * embedding_dimension) + embedding_index] = value_matrix_device_pointer[qkv_offset + (tile_size * column_block_index) + (thread_index_x * embedding_dimension) + embedding_index];\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        for (int row_block_index = 0; row_block_index < total_rows_in_blocks; row_block_index++)  {\n",
        "\n",
        "\n",
        "            for (int embedding_index = 0; embedding_index < embedding_dimension; embedding_index++) {\n",
        "                query_matrix_tile[(thread_index_x * embedding_dimension) + embedding_index] = query_matrix_device_pointer[qkv_offset + (tile_size * row_block_index) + (thread_index_x * embedding_dimension) + embedding_index];\n",
        "            }\n",
        "            float row_max_previous = max_matrix_device_pointer[lm_offset + (block_size_rows * row_block_index) + thread_index_x];\n",
        "            float row_sum_previous = sum_matrix_device_pointer[lm_offset + (block_size_rows * row_block_index) + thread_index_x];\n",
        "\n",
        "\n",
        "            float row_max = -INFINITY;\n",
        "            for (int column_index_inner = 0; column_index_inner < block_size_columns; column_index_inner++) {\n",
        "                float sum = 0;\n",
        "                for (int embedding_index = 0; embedding_index < embedding_dimension; embedding_index++) {\n",
        "                    sum += query_matrix_tile[(thread_index_x * embedding_dimension) + embedding_index] * key_matrix_tile[(column_index_inner * embedding_dimension) + embedding_index];\n",
        "                }\n",
        "                sum *= softmax_scale;\n",
        "                score_matrix_tile[(block_size_columns * thread_index_x) + column_index_inner] = sum;\n",
        "\n",
        "                if (sum > row_max)\n",
        "                    row_max = sum;\n",
        "            }\n",
        "\n",
        "            // probability_matrix_tile = exp(score_matrix_tile - row_max), row_sum = rowsum(probability_matrix_tile)\n",
        "            float row_sum = 0;\n",
        "            for (int column_index_inner = 0; column_index_inner < block_size_columns; column_index_inner++) {\n",
        "                score_matrix_tile[(block_size_columns * thread_index_x) + column_index_inner] = __expf(score_matrix_tile[(block_size_columns * thread_index_x) + column_index_inner] - row_max);\n",
        "                row_sum += score_matrix_tile[(block_size_columns * thread_index_x) + column_index_inner];\n",
        "            }\n",
        "\n",
        "            float row_max_new = max(row_max_previous, row_max);\n",
        "            float row_sum_new = (__expf(row_max_previous - row_max_new) * row_sum_previous) + (__expf(row_max - row_max_new) * row_sum);\n",
        "\n",
        "\n",
        "            // Write output_matrix_device_pointer, sum_matrix_device_pointer, max_matrix_device_pointer to HBM\n",
        "            for (int embedding_index = 0; embedding_index < embedding_dimension; embedding_index++) {\n",
        "                float probability_times_value = 0;  // Pij * Vj\n",
        "                for (int column_index_inner = 0; column_index_inner < block_size_columns; column_index_inner++) {\n",
        "                    probability_times_value += score_matrix_tile[(block_size_columns * thread_index_x) + column_index_inner] * value_matrix_tile[(column_index_inner * embedding_dimension) + embedding_index]+eps;\n",
        "                }\n",
        "                output_matrix_device_pointer[qkv_offset + (tile_size * row_block_index) + (thread_index_x * embedding_dimension) + embedding_index] = (1 / (eps+row_sum_new)) \\\n",
        "                    * ((row_sum_previous * __expf(row_max_previous - row_max_new) * output_matrix_device_pointer[qkv_offset + (tile_size * row_block_index) + (thread_index_x * embedding_dimension) + embedding_index]) \\\n",
        "                    + (__expf(row_max - row_max_new+eps) * probability_times_value));\n",
        "            }\n",
        "            max_matrix_device_pointer[lm_offset + (block_size_rows * row_block_index) + thread_index_x] = row_max_new;\n",
        "            sum_matrix_device_pointer[lm_offset + (block_size_rows * row_block_index) + thread_index_x] = row_sum_new;\n",
        "        }\n",
        "        __syncthreads();  // otherwise, thread can use the wrong key_matrix_tile, value_matrix_tile in inner loop\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "template <typename T>\n",
        "T* allocateAndInitializeDeviceMemory(size_t size, bool initializeToZero = false, bool initializeToNegativeInfinity = false) {\n",
        "    T* device_ptr;\n",
        "    cudaMalloc(&device_ptr, size); // No error checking\n",
        "\n",
        "    if (initializeToZero) {\n",
        "        cudaMemset(device_ptr, 0, size); // No error checking\n",
        "    } else if (initializeToNegativeInfinity) {\n",
        "        float negative_infinity_host = -INFINITY;\n",
        "        cudaMemset(device_ptr, *reinterpret_cast<int*>(&negative_infinity_host), size); // No error checking\n",
        "    } else {\n",
        "        curandGenerator_t generator;\n",
        "        curandCreateGenerator(&generator, CURAND_RNG_PSEUDO_DEFAULT); // No error checking\n",
        "        curandSetGeneratorOffset(generator, time(0)); // No error checking\n",
        "        curandGenerateUniform(generator, reinterpret_cast<float*>(device_ptr), size / sizeof(T)); // No error checking\n",
        "        curandDestroyGenerator(generator); // No error checking\n",
        "    }\n",
        "\n",
        "    return device_ptr;\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "void writeMatrixToFile(T* matrix, const std::string& filename, int batch_size, int num_heads, int sequence_length, int embedding_dimension) {\n",
        "    std::ofstream file(filename);\n",
        "    if (!file) {\n",
        "        std::cerr << \"Could not open the file!\" << std::endl;\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    for (int b = 0; b < batch_size; ++b) {\n",
        "        for (int h = 0; h < num_heads; ++h) {\n",
        "            for (int i = 0; i < sequence_length; ++i) {\n",
        "                for (int j = 0; j < embedding_dimension; ++j) {\n",
        "                    file << matrix[(b * num_heads * sequence_length * embedding_dimension) +\n",
        "                                   (h * sequence_length * embedding_dimension) +\n",
        "                                   (i * embedding_dimension) + j];\n",
        "                    if (j < embedding_dimension - 1) {\n",
        "                        file << \", \"; // Comma separation\n",
        "                    }\n",
        "                }\n",
        "                file << std::endl; // New line for next row\n",
        "            }\n",
        "            file << std::endl; // Extra new line for separating heads\n",
        "        }\n",
        "    }\n",
        "    file.close();\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "void printMatrix(T* matrix, int batch_size, int num_heads, int sequence_length, int embedding_dimension, int rowsToPrint, int colsToPrint) {\n",
        "    T* host_matrix = new T[batch_size * num_heads * sequence_length * embedding_dimension];\n",
        "    cudaMemcpy(host_matrix, matrix, batch_size * num_heads * sequence_length * embedding_dimension * sizeof(T), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::cout << \"Matrix:\\n\";\n",
        "    for (int b = 0; b < batch_size; ++b) {\n",
        "        for (int h = 0; h < num_heads; ++h) {\n",
        "            for (int i = 0; i < rowsToPrint; ++i) {\n",
        "                for (int j = 0; j < colsToPrint; ++j) {\n",
        "                    std::cout << host_matrix[(b * num_heads * sequence_length * embedding_dimension) +\n",
        "                                            (h * sequence_length * embedding_dimension) +\n",
        "                                            (i * embedding_dimension) + j] << \" \";\n",
        "                }\n",
        "                std::cout << std::endl;\n",
        "            }\n",
        "            std::cout << std::endl;\n",
        "        }\n",
        "    }\n",
        "    delete[] host_matrix;\n",
        "}\n",
        "/*\n",
        "void test_attention(int batch_size, int num_heads, int sequence_length, int embedding_dimension) {\n",
        "\n",
        "\n",
        "    // Generate random tensors for query, key, and value (similar to CUDA initialization)\n",
        "    auto query = torch::rand({batch_size, num_heads, sequence_length, embedding_dimension}, device);\n",
        "    auto key = torch::rand({batch_size, num_heads, sequence_length, embedding_dimension}, device);\n",
        "    auto value = torch::rand({batch_size, num_heads, sequence_length, embedding_dimension}, device);\n",
        "\n",
        "    // Calculate the softmax scale\n",
        "    float softmax_scale = 1.0f / std::sqrt(embedding_dimension);\n",
        "\n",
        "    // Prepare output and intermediate tensors\n",
        "    auto output = torch::zeros({batch_size, num_heads, sequence_length, embedding_dimension}, device);\n",
        "    auto sum_matrix = torch::zeros({batch_size, num_heads, sequence_length}, device);\n",
        "    auto max_matrix = torch::full({batch_size, num_heads, sequence_length}, -INFINITY, device);\n",
        "\n",
        "    // Perform attention operation (similar to your CUDA kernel logic)\n",
        "    for (int head = 0; head < num_heads; ++head) {\n",
        "        for (int seq_idx = 0; seq_idx < sequence_length; ++seq_idx) {\n",
        "            // Calculate attention scores\n",
        "            auto query_vec = query.index({0, head, seq_idx, torch::indexing::Slice()});\n",
        "            auto key_mat = key.index({0, head, torch::indexing::Slice(), torch::indexing::Slice()});\n",
        "\n",
        "            auto scores = torch::matmul(query_vec.unsqueeze(0), key_mat.transpose(1, 0)) * softmax_scale;\n",
        "\n",
        "            // Calculate softmax\n",
        "            auto max_score = std::get<0>(scores.max(1));\n",
        "            auto score_exp = torch::exp(scores - max_score.unsqueeze(1));\n",
        "\n",
        "            // Normalize\n",
        "            auto sum_score = score_exp.sum(1);\n",
        "            auto attention_weights = score_exp / (sum_score.unsqueeze(1) + 1e-10);  // Avoid division by zero\n",
        "\n",
        "            // Compute the output\n",
        "            auto value_mat = value.index({0, head, torch::indexing::Slice(), torch::indexing::Slice()});\n",
        "            auto weighted_value = torch::matmul(attention_weights.unsqueeze(1), value_mat).squeeze(1);\n",
        "\n",
        "            output.index({0, head, seq_idx, torch::indexing::Slice()}) = weighted_value;\n",
        "            max_matrix.index({0, head, seq_idx}) = max_score;\n",
        "            sum_matrix.index({0, head, seq_idx}) = sum_score;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Prints to validate outputs\n",
        "    std::cout << \"Query Tensor:\\n\" << query << \"\\n\";\n",
        "    std::cout << \"Key Tensor:\\n\" << key << \"\\n\";\n",
        "    std::cout << \"Value Tensor:\\n\" << value << \"\\n\";\n",
        "    std::cout << \"Output Tensor:\\n\" << output << \"\\n\";\n",
        "}\n",
        "*/\n",
        "int main() {\n",
        "\n",
        "    const int batch_size = 1;\n",
        "    const int num_heads = 1;\n",
        "    const int sequence_length = 64;\n",
        "    const int embedding_dimension = 64;\n",
        "\n",
        "\n",
        "    const int block_size_columns = 32;\n",
        "    const int block_size_rows = 32;\n",
        "\n",
        "    // Derived dimensions\n",
        "    const int total_columns_in_blocks = ceil((float)sequence_length / block_size_columns);\n",
        "    const int total_rows_in_blocks = ceil((float)sequence_length / block_size_rows);\n",
        "    const float softmax_scale = 1.0f / sqrtf(embedding_dimension);\n",
        "\n",
        "    // Calculate sizes for memory allocation\n",
        "    size_t matrix_size = batch_size * num_heads * sequence_length * embedding_dimension * sizeof(float);\n",
        "    size_t vector_size = batch_size * num_heads * sequence_length * sizeof(float);\n",
        "\n",
        "\n",
        "    // Device memory allocation and initialization using helper function\n",
        "    float* query_matrix_device = allocateAndInitializeDeviceMemory<float>(matrix_size);\n",
        "    float* key_matrix_device = allocateAndInitializeDeviceMemory<float>(matrix_size);\n",
        "    float* value_matrix_device = allocateAndInitializeDeviceMemory<float>(matrix_size);\n",
        "    float* output_matrix_device = allocateAndInitializeDeviceMemory<float>(matrix_size, true); // Initialize to zero\n",
        "    float* sum_matrix_device = allocateAndInitializeDeviceMemory<float>(vector_size, false, false);  // Initialize to zero\n",
        "    float* max_matrix_device = allocateAndInitializeDeviceMemory<float>(vector_size, false, true); // Initialize to -INFINITY\n",
        "cudaMemset(sum_matrix_device, 0, vector_size);\n",
        "\n",
        "\n",
        "\n",
        "    // Shared memory size calculation and check\n",
        "    const int shared_memory_size = (4* block_size_columns * embedding_dimension * sizeof(float)) +\n",
        "                                    (block_size_columns * block_size_rows * sizeof(float));\n",
        "    int max_shared_memory_size;\n",
        "    cudaDeviceGetAttribute(&max_shared_memory_size, cudaDevAttrMaxSharedMemoryPerBlock, 0);\n",
        "\n",
        "\n",
        "    // Kernel launch configuration\n",
        "    dim3 grid_dim(batch_size, num_heads);\n",
        "    dim3 block_dim(block_size_columns);\n",
        "\n",
        "    forward_kernel<<<grid_dim, block_dim, shared_memory_size>>>(\n",
        "        query_matrix_device, key_matrix_device, value_matrix_device, sequence_length,\n",
        "        embedding_dimension, total_columns_in_blocks, total_rows_in_blocks, block_size_columns,\n",
        "        block_size_rows, softmax_scale, sum_matrix_device, max_matrix_device, output_matrix_device);\n",
        "\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "\n",
        "    int rowsToPrint = sequence_length ;\n",
        "    int colsToPrint = embedding_dimension;\n",
        "\n",
        "\n",
        "\n",
        "    float* query_matrix_host = new float[batch_size * num_heads * sequence_length * embedding_dimension];\n",
        "    float* key_matrix_host = new float[batch_size * num_heads * sequence_length * embedding_dimension];\n",
        "    float* value_matrix_host = new float[batch_size * num_heads * sequence_length * embedding_dimension];\n",
        "    float* output_matrix_host = new float[batch_size * num_heads * sequence_length * embedding_dimension];\n",
        "\n",
        "    // Copy matrices from device to host\n",
        "    cudaMemcpy(query_matrix_host, query_matrix_device, matrix_size, cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(key_matrix_host, key_matrix_device, matrix_size, cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(value_matrix_host, value_matrix_device, matrix_size, cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(output_matrix_host, output_matrix_device, matrix_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Write each matrix to its respective output file\n",
        "    writeMatrixToFile(query_matrix_host, \"query_output.csv\", batch_size, num_heads, sequence_length, embedding_dimension);\n",
        "    writeMatrixToFile(key_matrix_host, \"key_output.csv\", batch_size, num_heads, sequence_length, embedding_dimension);\n",
        "    writeMatrixToFile(value_matrix_host, \"value_output.csv\", batch_size, num_heads, sequence_length, embedding_dimension);\n",
        "    writeMatrixToFile(output_matrix_host, \"output_output.csv\", batch_size, num_heads, sequence_length, embedding_dimension);\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    std::cout << \"Q:\\n\";\n",
        "    printMatrix(query_matrix_device, batch_size, num_heads, sequence_length, embedding_dimension, rowsToPrint, colsToPrint);\n",
        "    std::cout << \"K:\\n\";\n",
        "    printMatrix(key_matrix_device, batch_size, num_heads, sequence_length, embedding_dimension, rowsToPrint, colsToPrint);\n",
        "    std::cout << \"V:\\n\";\n",
        "    printMatrix(value_matrix_device, batch_size, num_heads, sequence_length, embedding_dimension, rowsToPrint, colsToPrint);\n",
        "    std::cout << \"O:\\n\";\n",
        "    printMatrix(output_matrix_device, batch_size, num_heads, sequence_length, embedding_dimension, rowsToPrint, colsToPrint);\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(query_matrix_device);\n",
        "    cudaFree(key_matrix_device);\n",
        "    cudaFree(value_matrix_device);\n",
        "    cudaFree(output_matrix_device);\n",
        "    cudaFree(sum_matrix_device);\n",
        "    cudaFree(max_matrix_device);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile with the specified architecture\n",
        "!nvcc vector_add.cu -o vector_add -gencode arch=compute_75,code=sm_75 -lcurand\n",
        "\n",
        "# Run the executable\n",
        "!./vector_add"
      ],
      "metadata": {
        "id": "QE_lVWRgmx4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWab4y4nm0HR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}