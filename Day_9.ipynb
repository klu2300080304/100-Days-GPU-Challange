{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIu6LcP2jaME"
      },
      "outputs": [],
      "source": [
        "%%writefile vector_add.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <math_constants.h>\n",
        "#include <cmath>\n",
        "\n",
        "#define SRAM_SIZE 1024                    // M: SRAM size\n",
        "#define sequence_length 2              // N: sequence length\n",
        "#define embed_dimension 2              // d: embedding dimension\n",
        "\n",
        "// Define constant sizes to be used for block sizes\n",
        "constexpr int Block_column_size = SRAM_SIZE / (4 * embed_dimension); // Bc\n",
        "constexpr int Block_row_size = std::min(SRAM_SIZE / (4 * embed_dimension), embed_dimension); // Br\n",
        "\n",
        "// Ensure we don't have a division by zero situation\n",
        "static_assert(Block_column_size > 0, \"Block_column_size must be greater than 0\");\n",
        "static_assert(Block_row_size > 0, \"Block_row_size must be greater than 0\");\n",
        "\n",
        "constexpr int Total_row_blocks = (sequence_length + Block_row_size - 1) / Block_row_size; // Tr\n",
        "constexpr int Total_column_blocks = (sequence_length + Block_column_size - 1) / Block_column_size; // Tc\n",
        "\n",
        "__global__ void flashAttentionForward(\n",
        "    const float *Query,                 // Q\n",
        "    const float *Key,                   // K\n",
        "    const float *Value,                 // V\n",
        "    float *Output,                      // O\n",
        "    float *max_values,                  // m\n",
        "    float *sum_values,                  // l\n",
        "    const float attention_scale)        // 1/sqrt(d)\n",
        "{\n",
        "    int thread_idx = threadIdx.x;\n",
        "\n",
        "    // Thread-local storage for attention scores and weights\n",
        "    float attention_scores[Block_row_size * Block_column_size];\n",
        "    float attention_weights[Block_row_size * Block_column_size];\n",
        "\n",
        "    // Shared memory blocks\n",
        "    float Query_block[Block_row_size * embed_dimension];\n",
        "    float Key_block[Block_column_size * embed_dimension];\n",
        "    float Value_block[Block_column_size * embed_dimension];\n",
        "\n",
        "    for (int col_block = 0; col_block < Total_column_blocks; ++col_block)\n",
        "    {\n",
        "        // Load Key and Value blocks from global memory\n",
        "        if (thread_idx < Block_column_size) {\n",
        "            for (int d = 0; d < embed_dimension; ++d) {\n",
        "                Key_block[thread_idx * embed_dimension + d] =\n",
        "                    Key[col_block * Block_column_size * embed_dimension + thread_idx * embed_dimension + d];\n",
        "                Value_block[thread_idx * embed_dimension + d] =\n",
        "                    Value[col_block * Block_column_size * embed_dimension + thread_idx * embed_dimension + d];\n",
        "            }\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        for (int row_block = 0; row_block < Total_row_blocks; ++row_block)\n",
        "        {\n",
        "            if (thread_idx < Block_row_size) {\n",
        "                // Load Query block\n",
        "                for (int d = 0; d < embed_dimension; ++d) {\n",
        "                    Query_block[thread_idx * embed_dimension + d] =\n",
        "                        Query[row_block * Block_row_size * embed_dimension + thread_idx * embed_dimension + d];\n",
        "                }\n",
        "            }\n",
        "            __syncthreads();\n",
        "\n",
        "            // Compute attention scores for this row\n",
        "            if (thread_idx < Block_row_size) {\n",
        "                float row_max = -1e20;  // Use a large negative float\n",
        "                for (int k = 0; k < Block_column_size; ++k) {\n",
        "                    float score = 0.0f;\n",
        "                    for (int d = 0; d < embed_dimension; ++d) {\n",
        "                        score += Query_block[thread_idx * embed_dimension + d] *\n",
        "                                Key_block[k * embed_dimension + d];\n",
        "                    }\n",
        "                    score *= attention_scale;\n",
        "                    attention_scores[thread_idx * Block_column_size + k] = score;\n",
        "                    row_max = fmaxf(row_max, score);\n",
        "                }\n",
        "\n",
        "                // Compute attention weights with softmax\n",
        "                float row_sum = 0.0f;\n",
        "                for (int k = 0; k < Block_column_size; ++k) {\n",
        "                    float weight = expf(attention_scores[thread_idx * Block_column_size + k] - row_max);\n",
        "                    attention_weights[thread_idx * Block_column_size + k] = weight;\n",
        "                    row_sum += weight;\n",
        "                }\n",
        "\n",
        "                // Update output\n",
        "                for (int d = 0; d < embed_dimension; ++d) {\n",
        "                    float weighted_sum = 0.0f;\n",
        "                    for (int k = 0; k < Block_column_size; ++k) {\n",
        "                        weighted_sum += attention_weights[thread_idx * Block_column_size + k] *\n",
        "                                      Value_block[k * embed_dimension + d];\n",
        "                    }\n",
        "                    Output[row_block * Block_row_size * embed_dimension + thread_idx * embed_dimension + d] =\n",
        "                        (row_sum > 0) ? (weighted_sum / row_sum) : 0; // Avoid division by zero\n",
        "                }\n",
        "            }\n",
        "            __syncthreads();\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    // Host memory allocation\n",
        "    float (*Query)[embed_dimension] = new float[sequence_length][embed_dimension];\n",
        "    float (*Key)[embed_dimension] = new float[sequence_length][embed_dimension];\n",
        "    float (*Value)[embed_dimension] = new float[sequence_length][embed_dimension];\n",
        "    float (*Output)[embed_dimension] = new float[sequence_length][embed_dimension];\n",
        "    float *sum_values = new float[sequence_length]();  // Initialize to zeros\n",
        "    float *max_values = new float[sequence_length];\n",
        "\n",
        "    // Initialize max_values to a very small negative number\n",
        "    for (int i = 0; i < sequence_length; i++) {\n",
        "        max_values[i] = -1e20;  // Large negative float\n",
        "    }\n",
        "\n",
        "    // Initialization (random values between -1 and 1):\n",
        "    for (int i = 0; i < sequence_length; i++) {\n",
        "        for (int j = 0; j < embed_dimension; j++) {\n",
        "            Query[i][j] = 2.0f * rand() / RAND_MAX - 1.0f;\n",
        "            Key[i][j] = 2.0f * rand() / RAND_MAX - 1.0f;\n",
        "            Value[i][j] = 2.0f * rand() / RAND_MAX - 1.0f;\n",
        "            Output[i][j] = 0.0f;  // Initialize output to zeros\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Device memory pointers\n",
        "    float *device_Query, *device_Key, *device_Value, *device_Output;\n",
        "    float *device_max_values, *device_sum_values;\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc(&device_Query, sequence_length * embed_dimension * sizeof(float));\n",
        "    cudaMalloc(&device_Key, sequence_length * embed_dimension * sizeof(float));\n",
        "    cudaMalloc(&device_Value, sequence_length * embed_dimension * sizeof(float));\n",
        "    cudaMalloc(&device_Output, sequence_length * embed_dimension * sizeof(float));\n",
        "    cudaMalloc(&device_sum_values, sequence_length * sizeof(float));\n",
        "    cudaMalloc(&device_max_values, sequence_length * sizeof(float));\n",
        "\n",
        "    // Copy data from host to device\n",
        "    cudaMemcpy(device_Query, Query,\n",
        "               sequence_length * embed_dimension * sizeof(float),\n",
        "               cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(device_Key, Key,\n",
        "               sequence_length * embed_dimension * sizeof(float),\n",
        "               cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(device_Value, Value,\n",
        "               sequence_length * embed_dimension * sizeof(float),\n",
        "               cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(device_Output, Output,\n",
        "               sequence_length * embed_dimension * sizeof(float),\n",
        "               cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(device_sum_values, sum_values,\n",
        "               sequence_length * sizeof(float),\n",
        "               cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(device_max_values, max_values,\n",
        "               sequence_length * sizeof(float),\n",
        "               cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Calculate attention scaling factor\n",
        "    float attention_scale = 1.0f / sqrt(embed_dimension);\n",
        "\n",
        "    // Launch configuration\n",
        "    dim3 block_dim(Block_row_size);  // One thread per row in Query block\n",
        "    dim3 grid_dim(1);                // Single block for simplicity\n",
        "\n",
        "    // Launch kernel\n",
        "    flashAttentionForward<<<grid_dim, block_dim>>>(\n",
        "        device_Query,\n",
        "        device_Key,\n",
        "        device_Value,\n",
        "        device_Output,\n",
        "        device_max_values,\n",
        "        device_sum_values,\n",
        "        attention_scale\n",
        "    );\n",
        "\n",
        "    // Check for kernel launch errors\n",
        "    cudaError_t cudaStatus = cudaGetLastError();\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        fprintf(stderr, \"Kernel launch failed: %s\\n\", cudaGetErrorString(cudaStatus));\n",
        "        goto Error;\n",
        "    }\n",
        "\n",
        "    // Copy results back to host\n",
        "    cudaMemcpy(Output, device_Output,\n",
        "               sequence_length * embed_dimension * sizeof(float),\n",
        "               cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(max_values, device_max_values,\n",
        "               sequence_length * sizeof(float),\n",
        "               cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(sum_values, device_sum_values,\n",
        "               sequence_length * sizeof(float),\n",
        "               cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "\n",
        "// Print Query\n",
        "std::cout << \"Query:\" << std::endl;\n",
        "for (int i = 0; i < sequence_length; i++) {\n",
        "    for (int j = 0; j < embed_dimension; j++) {\n",
        "        std::cout << Query[i][j] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "}\n",
        "\n",
        "// Print Key\n",
        "std::cout << \"Key:\" << std::endl;\n",
        "for (int i = 0; i < sequence_length; i++) {\n",
        "    for (int j = 0; j < embed_dimension; j++) {\n",
        "        std::cout << Key[i][j] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "}\n",
        "\n",
        "// Print Value\n",
        "std::cout << \"Value:\" << std::endl;\n",
        "for (int i = 0; i < sequence_length; i++) {\n",
        "    for (int j = 0; j < embed_dimension; j++) {\n",
        "        std::cout << Value[i][j] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "}\n",
        "\n",
        "// Print Output\n",
        "std::cout << \"Output:\" << std::endl;\n",
        "for (int i = 0; i < sequence_length; i++) {\n",
        "    for (int j = 0; j < embed_dimension; j++) {\n",
        "        std::cout << Output[i][j] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "}\n",
        "\n",
        "Error:\n",
        "    // Cleanup\n",
        "    // Device memory\n",
        "    cudaFree(device_Query);\n",
        "    cudaFree(device_Key);\n",
        "    cudaFree(device_Value);\n",
        "    cudaFree(device_Output);\n",
        "    cudaFree(device_max_values);\n",
        "    cudaFree(device_sum_values);\n",
        "\n",
        "    // Host memory\n",
        "    delete[] Query;\n",
        "    delete[] Key;\n",
        "    delete[] Value;\n",
        "    delete[] Output;\n",
        "    delete[] sum_values;\n",
        "    delete[] max_values;\n",
        "\n",
        "    return cudaStatus == cudaSuccess ? 0 : 1;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile with the specified architecture\n",
        "!nvcc vector_add.cu -o vector_add -gencode arch=compute_75,code=sm_75\n",
        "\n",
        "# Run the executable\n",
        "!./vector_add"
      ],
      "metadata": {
        "id": "6TVsRPr0jf5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lKsXbmZvjkyS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}