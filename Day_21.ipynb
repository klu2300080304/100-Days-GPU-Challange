{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlfmvHjp2J29"
      },
      "outputs": [],
      "source": [
        "%%writefile vector_add.cu\n",
        "\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <curand_kernel.h>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// CUDA kernel to compute predictions and squared loss\n",
        "__global__ void compute_loss(float* X, float* y, float* W, float* b, float* loss, float* y_pred, int N, int D) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < N) {\n",
        "        float y_pred_val = 0.0f;\n",
        "        for (int i = 0; i < D; i++) {\n",
        "            y_pred_val += X[idx * D + i] * W[i];\n",
        "        }\n",
        "        y_pred_val += *b; // Use single scalar bias\n",
        "        y_pred[idx] = y_pred_val;\n",
        "        loss[idx] = (y[idx] - y_pred_val) * (y[idx] - y_pred_val); // Squared loss\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA kernel to compute gradients\n",
        "__global__ void compute_gradients(float* X, float* loss, float* dW, float* db, int N, int D) {\n",
        "    __shared__ float db_shared[BLOCK_SIZE];\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < D) {\n",
        "        float gradW = 0.0f;\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            gradW += X[i * D + idx] * loss[i];\n",
        "        }\n",
        "        dW[idx] = - (2.0f / N) * gradW;\n",
        "    }\n",
        "    float gradb = 0.0f;\n",
        "    if (idx < N) {\n",
        "        gradb = loss[idx];\n",
        "    }\n",
        "    db_shared[threadIdx.x] = gradb;\n",
        "    __syncthreads();\n",
        "\n",
        "    if (threadIdx.x == 0) {\n",
        "        float sum_db = 0.0f;\n",
        "        for (int i = 0; i < blockDim.x; i++) {\n",
        "            sum_db += db_shared[i];\n",
        "        }\n",
        "        atomicAdd(db, - (2.0f / N) * sum_db);\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA kernel to update weights using SGD\n",
        "__global__ void update_weights(float* W, float* dW, float* b, float* db, float lr, int D) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < D) {\n",
        "        W[idx] -= lr * dW[idx];\n",
        "    }\n",
        "    if (idx == 0) {\n",
        "        *b -= lr * (*db);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Host function to train the model\n",
        "void train_sgd(float* h_X, float* h_y, float* h_W, float* h_b, int N, int D, float lr, int epochs) {\n",
        "    float *d_X, *d_y, *d_W, *d_b, *d_gradW, *d_gradb, *d_loss, *d_y_pred;\n",
        "    cudaMalloc(&d_X, N * D * sizeof(float));\n",
        "    cudaMalloc(&d_y, N * sizeof(float));\n",
        "    cudaMalloc(&d_W, D * sizeof(float));\n",
        "    cudaMalloc(&d_b, sizeof(float));\n",
        "    cudaMalloc(&d_gradW, D * sizeof(float));\n",
        "    cudaMalloc(&d_gradb, sizeof(float));\n",
        "    cudaMalloc(&d_loss, N * sizeof(float));\n",
        "    cudaMalloc(&d_y_pred, N * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_X, h_X, N * D * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_y, h_y, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_W, h_W, D * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int blocks = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "    int blocks_grad = (D + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    for (int epoch = 0; epoch < epochs; ++epoch) {\n",
        "        compute_loss<<<blocks, BLOCK_SIZE>>>(d_X, d_y, d_W, d_b, d_loss, d_y_pred, N, D);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        compute_gradients<<<blocks_grad, BLOCK_SIZE>>>(d_X, d_loss, d_gradW, d_gradb, N, D);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        update_weights<<<blocks_grad, BLOCK_SIZE>>>(d_W, d_gradW, d_b, d_gradb, lr, D);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(h_W, d_W, D * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(h_b, d_b, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaFree(d_X);\n",
        "    cudaFree(d_y);\n",
        "    cudaFree(d_W);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_gradW);\n",
        "    cudaFree(d_gradb);\n",
        "    cudaFree(d_loss);\n",
        "    cudaFree(d_y_pred);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1024;\n",
        "    int D = 10;\n",
        "    float lr = 0.01;\n",
        "    int epochs = 1000;\n",
        "\n",
        "    float *h_X = new float[N * D];\n",
        "    float *h_y = new float[N];\n",
        "    float *h_W = new float[D];\n",
        "    float *h_b = new float[1];\n",
        "\n",
        "    srand(42);\n",
        "    for (int i = 0; i < N * D; i++) {\n",
        "        h_X[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "    }\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_y[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "    }\n",
        "    for (int i = 0; i < D; i++) {\n",
        "        h_W[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "    }\n",
        "    *h_b = static_cast<float>(rand()) / RAND_MAX;\n",
        "\n",
        "    train_sgd(h_X, h_y, h_W, h_b, N, D, lr, epochs);\n",
        "\n",
        "    std::cout << \"Trained Weights: \";\n",
        "    for (int i = 0; i < D; i++) std::cout << h_W[i] << \" \";\n",
        "    std::cout << \"\\nTrained Bias: \" << *h_b << std::endl;\n",
        "\n",
        "    delete[] h_X;\n",
        "    delete[] h_y;\n",
        "    delete[] h_W;\n",
        "    delete[] h_b;\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile with the specified architecture\n",
        "!nvcc vector_add.cu -o vector_add -gencode arch=compute_75,code=sm_75\n",
        "\n",
        "# Run the executable\n",
        "!./vector_add"
      ],
      "metadata": {
        "id": "kqwLDhYe2RSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v30x27Ut2TKn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}